% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
%\input{helper}
\usepackage{color}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\begin{document}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


\title{The  Sloan Digital Sky Survey Datasets}
\numberofauthors{1} 
\author{
Haoran Yu
}


\maketitle

%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\section{Data Description}
Sloan Digital Sky Survey (SDSS) is an astronomy project that  aims to provide a digital map of the sky~\cite{szalay2002sdss}.  SDSS data includes raw images of the sky and measurements known as \textit{scientific attributes}, which are numerical estimates of the  physical attributes of  objects in the images~\cite{szalay2002sdss}.   The scientific attributes are  stored in a database system called the the Catalog Archive Server (CAS) database. Access to the CAS data is provided through several data access interfaces, including the SkySever website, which provides a web-based interface for writing SQL queries. There are also  SQL templates provided by SkyServer to assist users in constructing queries \cite{raddick2014ten1,szalay2002sdss}.
 %The celestial objects and their attributes are stored in SQL Server/ObjectivityDB backend database servers along with the captured images and spectrographs \cite{szalay2002sdss}.
%Front end portals to access the SDSS data is SkyServer, where it offers a web interface for web browsers and also allows users to submit raw SQL queries for SDSS data \cite{szalay2002sdss}. 
%. 

SQL queries submitted via the access interfaces to the CAS are logged and  provided in the SkyServer logs data~\cite{singh2007skyserver,raddick2014ten1,raddick2014ten2}. These logs include information about the IP address that submitted the query, the web agent, the time stamp, the SQL query statement, and the version of the database that it queried~\cite{szalay2002sdss}. 

%Access patterns of the web interface and SQL query interface of SkyServer have been studied by parsing the logs \cite{hirota2016mining}. Website traffic have been studied by Singh \cite{singh2007skyserver} and Raddick \cite{raddick2014ten1}, and preliminary studies of SQL query pattern were \textcolor{red}{done in} \cite{hirota2016mining}. 

Generally, there are 3 datasets associated with the SDSS project:
\footnote{The report should  clear, concise, accurate, and easy to follow} 
\footnote{This section should very briefly introduce the data.  I've rewritten this part to convey only the important information..}
\footnote{ Regarding references, don't name authors, only reference their paper.  Also, please check grammar throughout please.}
\footnote{Before you had just mentioned the scientific attributes. Remember, any new concept that you introduce should be defined, along with a reference to the paper that defined it originally.}
\footnote{You need to put labels for the figures, then you refer to the labels in the tex files, rather than name the figures in the text. There is no point in introducing the figures here, since  they were not described in detail, the reader doesn't gain anything from looking at the figures at this point.}
\footnote{for the datasets below, we need statistics for the most recent release of the data. So, for the CAS database, you need to say which release the schema corresponds to and how many tables, attributes, etc it contains. Similarly for the rest of the datasets.}

\noindent \textbf{CAS database. } The CAS dataset stores the scientific attributes of celestial objects identified in the captured photos, the corresponding photos can be retrieved from the Data Archive Server (DAS) at the Fermilab physical sciences laboratory  \cite{szalay2002sdss} The schema for the original CAS database is shown  in Figure~\ref{fig:schemaCAS}\footnote{For all these figures, you need to cite the original paper you took it from}. SDSS releases new versions of data yearly to update and expand the astronomy data, each version is called a Data Release (DR). A recent data release (Data Release 12) has expanded the original CAS database schema to 115 tables, and offers 59 views, 231 functions and 21 procedures for queries \cite{raddick2014ten1}. In DR12, tables such as PhotoObjAll have over 350 table attributes and has as many as 790 million tuples \cite{cas_dr12}. Among the tables, PhotoObjAll, SpecObjAll, and Profile store the primary scientific attributes of the photos and spectrographs of the celestial objects. 

\noindent \textbf{SkyServer logs dataset. }The SkyServer logs dataset aggregates each attempt to access the web interfaces and submission of SQL queries since 2001 and is publicly accessible \cite{singh2007skyserver, raddick2014ten1, raddick2014ten2}. \footnote{The have three papers on this, first 5 years, then two on the first 10 years. You need to cite all three here.} The schema for this dataset is shown in Figure~\ref{fig:schemaSSLogsDB} . Logs are harvested from the server every hour and stored in 7 tables \cite{singh2007skyserver}. The WebLog and SqlLog tables, which store each web page visit and each SQL query submitted, can be accessed online\footnote{All the tables are accessible, why this statement? the web interface allows simple select queries on the two tables, but you have access to all the data if you download it.}. The web logs have recorded 630 million webpage views\cite{raddick2014ten2} and has logged over 330 million submitted SQL queries \cite{hirota2016mining} from 2001 to 2011. 

\noindent \textbf{Normalized SkyServer logs. }The third dataset is a subset of the SkyServer logs dataset storing data from 2001 to 2011, this dataset is normalized to reduce its storage size\cite{raddick2014ten1}. The normalization process involves moving common items such as IP addresses in WebLog and query statements in SqlLog to a separate table, and replacing the items in the original WebLog and SqlLog with foreign keys to reference the items in the separate tables\cite{raddick2014ten1}. The schema for this dataset is shown in Figure~\ref{fig:schemaSSNormalizedLogs}. These separate tables are called ancillary tables, such as ipDomain, session, Webagent, and SqlStatement. The modified dataset is publicly accessible and was used for SkyServer traffic analysis and SQL usage analysis \cite{singh2007skyserver, raddick2014ten1}. There are 14 tables in total, and all tables have less than 30 attributes \cite{raddick2014ten1}. The normalized logs were later expanded to include data until 2015 in \cite{hirota2016mining} but is not publicly available, therefore we will use dataset until 2011 to perform further SQL usage analysis. 

\begin{table*}
\centering
\caption{Summary of the 3 SDSS Datasets (approximation only)}
\begin{tabular}{|l|l|l|l|} \hline
Dataset&Num of Tables&Max Table Attributes&Max Table Tuples\\ \hline
CAS & 115& approx. 350&approx. 790 million\\ \hline
SkyServer Logs & 7 & approx. 20& approx. 630 million\\ \hline
Normalized SkyServer Logs & 14 & approx. 30& approx. 630 million\\ \hline
\end{tabular}
\end{table*}

\begin{figure*}
\centering
\includegraphics[width=0.75\textwidth,height=\textheight,keepaspectratio]{Figures/schema1}
\caption{Schema of the Catalog Archive Server (CAS) Database. Image taken from \cite{szalay2002sdss}}
\label{fig:schemaCAS}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.75\textwidth,height=\textheight,keepaspectratio]{Figures/schema2}
\caption{Schema of SkyServer Logs Database. Image taken from \cite{raddick2014ten1}}
\label{fig:schemaSSLogsDB}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.75\textwidth,height=\textheight,keepaspectratio]{Figures/schema3}
\caption{Schema of SkyServer Normalized Logs. Image taken from \cite{raddick2014ten1}}
\label{fig:schemaSSNormalizedLogs}
\end{figure*}

\section{Sessions}
\textcolor{blue}{Summary of procedure for data processing in 10 years paper-1: 
Client IP addresses and agent strings: First, agent strings in the logs are associated with Web agents. These web agents are then classified into 5 classes: anonymous, bot, administrative service, user program, or Web browser. This is stored in the agent table. 
Then, the domains of the IP addresses in the web hits are resolved, and categorized into ~1047 domain organizations. The domain organizations are further categorized into: research institution, university, college, community college, k-12 institution, ISP, non-ISP, and government organization. (this info may not be available to us.)}

According to~\cite{raddick2014ten1}, a \textit{hit}  ``comes from a Web browser, or a program that the user has written to download data, and has a command requesting a file type that delivers information to the end user. '' \footnote{\textcolor{red}{Based on this definition, it seems only hits from user program and web browser information are logged. }}
For weblogs , a hit is a pageview, and each row in a weblog is a hit. A pageview is indicated in the normalized weblog table with a boolean flag variable. 
For SQL query logs, a hit is a query. 

As defined by~\cite{raddick2014ten1}, a \textit{session} is an ordered sequence of hits from a single IP address, such that the gaps between hits in the sequence is no longer than 30 minutes. If an additional hit occurs after the 30 minutes gap for that IP, a new session is declared. 


\subsection{Tracing a Session} 
To study the SQL usage pattern by a session, we will trace each session to reconstruct the original queries that were submitted. This can be achieved by joining multiple tables in the normalized logs dataset. For example, a session table defines individual sessions and the sessionLogs table stores all requests of all sessions with the rankInSession attribute that orders all requests of each session sequentially. sessionLogs table also stores the necessary pointers to the WebLog and SqlLog tables to retrieve the original query. Studying the results returned by each query assists our usage pattern analysis. However, since the SqlLog does not store the results returned by submitting the query - it only stores the number of rows returned - we will need submit the query to Skyserver to retrieve the query results.

We provide an example for a session, that is, a sequence of hits/queries submitted by a user.  We traced the session, and obtained the hits/queries of the session in the table below. 

\begin{table*}
\centering
\caption{Summary of tracing a session with sessionID 100000 using the normalized Skyserver logs}
\begin{tabular}{llllll}
sessionID & rankInSession & sessionAddressString & theTime                 & statementID & rows \\
100000    & 1             & 155.198.204.142      & 2008-04-21,13:04:05.000 & 48063987    & 1    \\
100000    & 2             & 155.198.204.142      & 2008-04-21,13:04:06.000 & 48063987    & 1    \\
100000    & 3             & 155.198.204.142      & 2008-04-21,13:04:10.000 & 48063987    & 1    \\
100000    & 4             & 155.198.204.142      & 2008-04-21,13:04:15.000 & 48063987    & 1    \\
100000    & 5             & 155.198.204.142      & 2008-04-21,13:04:15.000 & 48063987    & 1    \\
100000    & 6             & 155.198.204.142      & 2008-04-21,13:04:22.000 & 48063987    & 1    \\
100000    & 7             & 155.198.204.142      & 2008-04-21,13:04:26.000 & 48390699    & 51   \\
100000    & 8             & 155.198.204.142      & 2008-04-21,13:04:34.000 & 48390699    & 51   \\
100000    & 9             & 155.198.204.142      & 2008-04-21,13:04:34.000 & 48063987    & 1    \\
100000    & 10            & 155.198.204.142      & 2008-04-21,13:04:40.000 & 48063987    & 1    \\
100000    & 11            & 155.198.204.142      & 2008-04-21,13:04:58.000 & 48063987    & 1   
\end{tabular}
\end{table*}

The method used to trace a session is described in pseudo-SQL. Here, we trace a session with sessionID 100000.

\begin{lstlisting}
--TRACING A SESSION--
--Query1: find the IP address of a session sessionIpID, and how many queries were made in that session sqlqueries
select sessionID, sqlqueries, sessionIpID
into TEMP_TABLE1
from session.csv
where sessionID = 100000;

--Query2: find the sequence of the queries made in a session where each query is indicated by a rank rankInSession, and get the time that the query was made theTime.
select sl.sessionID, sl.rankInSession, sl.theTime
into TEMP_TABLE2
from sessionlog.csv
join TEMP_TABLE1 t1
where sl.sessionID = t1.sessionID;

--Query3: confirm that this IP is valid
select ip.ipID, ip.isvalid
into TEMP_TABLE3
from ipAll.csv ip
join TEMP_TABLE1 t1
where ip.ipID = t1.sessionIpID;

--Query4: get the ID of the SQL query statement statementID for each query made in a session. 
--Also get the number of rows returned by submitting a particular query. 
--Get the access interface i.e. "casjobs" or "public" that the request was sent.
--NOTE theTime in TEMP_TABLE2 and in sqllog.csv are in different granularities, need to preprocess TEMP_TABLE2 first
select sl.clientIpID, sl.theTime, sl.statementID, sl.rows, sl.logID, sl.access
into TEMP_TABLE4
from sqllog.csv sl
join TEMP_TABLE2 t2
join TEMP_TABLE1 t1
where sl.clientIpID = t1.sessionIpID
and sl.theTime = t2.theTime

--Query 5: get the SQL query statement identified by the statementID
select ss.statementID, ss.statement
into TEMP_TABLE5
from sqlstatement.csv ss
join TEMP_TABLE4 t4
where t4.statementID = ss.statementID


--FINDING THE WEBAGENT OF THE QUERY--
--Query: get the class of the web agent string, BOT, or ADMIN, etc.
--NOTE: this query does not work! No weblog.csv row with the correct time corresponding to sqllog.csv can be found
select ls.logID, wl.agentStringID, was.agentID, wa.class
into TEMP_TABLE_AGENT
from LogSource.csv ls
join TEMP_TABLE4 t4
join weblog.csv wl
join weagentstring.csv was
join WebAgent.csv wa
where t4.logID = ls.logID
and ls.logID = wl.logID
and wl.agentStringID = was.agentID
and was.agentID = wa.agentID
\end{lstlisting}

After tracing a session with sessionID 100000, the following SQL query statements were retrieved from the logs.

\begin{lstlisting}
--statementID 48063987
SELECT u.up_name as name, 
   '<a target=INFO href=http://cas.sdss.org/astrodr6/en/tools/explore/obj.asp?id=' + cast(x.objId as varchar(20)) + '>'+ cast(x.objId as varchar(20)) + '</a>' as objID, p.ra, p.dec, 
   dbo.fPhotoTypeN(p.type) as type,
   p.modelMag_u, p.modelMag_g, p.modelMag_r, p.modelMag_i, p.modelMag_z, p.modelMagErr_u, p.modelMagErr_g, p.modelMagErr_r, p.modelMagErr_i, p.modelMagerr_z, p.z, p.zErr
FROM #x x, #upload u, SpecPhotoAll p
WHERE u.up_id = x.up_id and x.objID=p.objID 
ORDER BY x.up_id

--statementID 48390699
SELECT u.up_name as name, 
   '<a target=INFO href=http://cas.sdss.org/astrodr6/en/tools/explore/obj.asp?id=' + cast(x.objId as varchar(20)) + '>'+ cast(x.objId as varchar(20)) + '</a>' as objID, p.ra, p.dec, 
   dbo.fPhotoTypeN(p.type) as type,
   p.modelMag_u, p.modelMag_g, p.modelMag_r, p.modelMag_i, p.modelMag_z, p.modelMagErr_u, p.modelMagErr_g, p.modelMagErr_r, p.modelMagErr_i, p.modelMagerr_z
FROM #x x, #upload u, PhotoTag p
WHERE u.up_id = x.up_id and x.objID=p.objID
ORDER BY x.up_id
\end{lstlisting}


\textcolor{blue}{Questions to address : \begin{itemize}
\item How can we  group the sql query logs based on the access interface they were submitted through? e.g, CASjobs or SkyServer web-based interface.  Do we need to filter the queries to select those that come from the web-based access interface only? 
\item Can we distinguish between logs based on their agent strings, and should we do this, since, based on the definition of hit,  it seems only hits from user programs and web browser information are logged.
\end{itemize} 
How to trace the path that users follow?  query the two tables:
\begin{itemize}
\item sessionlogs table: combined weblog and Sql log data sorted by client IP address and then time. IDs of logs should be here.
\item The sessions table: information about each session, including client IP address, start and end time, and the number of web hits and SQL queries in that session.
\end{itemize} }












\subsection{Users vs Sessions}
\footnote{I kept this section for now, but it may not be needed}
To study SQL usage patterns, we are interested in observing the way human users construct queries, and the way they think and learn. %However, many of the queries submitted are from automated programs, and humans can also use different IP addresses, \textcolor{red}{therefore human users are hard to characterize. The heuristic approach} that Singh \cite{singh2007skyserver} and Raddick \cite{raddick2014ten1} took was to \textcolor{red}{view users as sessions}.
Many large studies have used this heuristic approach and Singh \cite{singh2007skyserver} confirmed that 98 percent of all IPs do not send requests after a 30 minutes gap. From the study by Singh \cite{singh2007skyserver}, there are about 1 million unique IP addresses and 3 million sessions for the Skyserver web interface, and 20,000 unique IP addresses and 100,000 sessions for the SQL interface. The number of unique IP addresses have increased to 3 million 5 years later in the study by Raddick \cite{raddick2014ten1}. Raddick observed that while 1 million IP addresses have only one pageview, research-intensive institutions such as UC Berkeley have 10 million pageviews \cite{raddick2014ten1}. We are going to use the same heuristic approach to study user SQL usage patterns. 





\section{Related Work} \footnote{Maybe this section should be called related work.}
Many interesting usage characteristics can be drawn from the normalized logs dataset. Singh \cite{singh2007skyserver} and Raddick \cite{raddick2014ten1} plotted frequency of various items, such as pageviews, against time to observe change of certain patterns over time. In addition, Singh \cite{singh2007skyserver} looked at Rank-Size Distribution of SQL query words of the SQL Template provided by SkyServer and observed it corresponds to Zipf's Law, and Raddick observed a faster exponential decrease of frequency as rank decreases. At the same time, using the Rank-Size Distribution of the number of requests per session, Singh observed the Zipfian distribution as well \cite{singh2007skyserver}. Hirota \cite{makiyama2015text} performed various statistical tests on the parsed logs such as the K-Means algorithm to partition the query statements in groups and the hierarchical clustering algorithm to cluster the statements. However, no studies observed any pattern of the results returned by submitting the queries. 

Since we are interested in obtaining the user-item interaction data of SQL requests, we will be studying the 68 million unique queries in the normalized dataset by Raddick \cite{raddick2014ten1}. \textcolor{red}{As outlined in Hirota \cite{makiyama2015text}, only queries made through SkyServer, not CASJobs, should be used since all queries in CASJobs are submitted by automated programs; these are unique queries made by human users that are valid and returned tuples.} In Hirota's study \cite{hirota2016mining}, each item is a token parsed from SQL statements. Tokens are a group of words where each token associates the SQL keywords to table names, column attributes, or other SQL keywords \cite{hirota2016mining}. Examples such as \verb|`select_elliptical'|, \verb|`select_objid'|, \verb|`from_bestobjid'|, \verb|`from_inner'|, \verb|`from_join'|, \verb|`from_photoobj'|, \verb|`where_ra'|, \verb|`where_u'| are some of the tokens parsed from a SQL query statement \cite{makiyama2015text}. Notice the metadata attached to each token, such as attaching `select' to `objid' to become \verb|`select_objid'| to indicate that `objid' is within the `select' clause. Hirota then converted the parsed queries into feature vectors, where the values in this vector are the frequency of specific tokens \cite{makiyama2015text}. Values of the vectors were transformed using the TF*IDF weighting scheme \cite{makiyama2015text}.


\section{Project Goals}
 For the first part of the project, loading the data into Python class objects, submitting the query to the server as CaJobs, and retrieve the results returned. 

Goals in project, for now:
\begin{itemize}
\item 21 Sep: Download the data. 
\item 24 Sep: Re-iterate on this document, address comments, and verify/elaborate ambiguous parts.
\item 26 Sep: Trace a single user session and explain what has happened. Explain that user session, use figures if needed.
\item 27 Sep: How can we obtain the results for the queries the user submitted?
\item 7 Oct: Construct the session-item matrix:
\begin{itemize}
\item Design a scheme for parsing the data, with my help. 
\item Write the code, with my help. 
\end{itemize} 

\end{itemize}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{ref}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%

\end{document}
